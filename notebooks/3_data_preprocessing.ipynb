{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f326ffa3",
   "metadata": {},
   "source": [
    "# Data Preprocessing \n",
    "\n",
    "Data preprocessing is a crucial step in any machine learning workflow. It involves cleaning, transforming, and preparing the raw data to make it suitable for model training. In this notebook, we will focus on the following preprocessing steps:\n",
    "\n",
    "1.  **Loading the data**: We will load the raw data from CSV files using pandas.\n",
    "2.  **Splitting the data**: We will split the data into training and validation sets to evaluate the model's performance.\n",
    "3.  **Feature Engineering**: We will create new features from the existing ones to improve the model's accuracy. This will involve domain-driven feature engineering, where we use our knowledge of the problem to create meaningful features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6b3819",
   "metadata": {},
   "source": [
    "### 1. Load & Split the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc71adb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (1600, 55)\n",
      "Validation data shape: (400, 55)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Directory of the datasets\n",
    "data_path = Path('../raw_data')\n",
    "\n",
    "# Load the raw dataset\n",
    "train_data = None\n",
    "try:\n",
    "    train_data = pd.read_csv(data_path / 'train.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found, recheck the directory or change the path of directory.\")\n",
    "\n",
    "if train_data is not None:\n",
    "    # Feature & target columns\n",
    "    feature_cols = [col for col in train_data.columns if 'Component' in col]\n",
    "    target_cols = [col for col in train_data.columns if 'Blend' in col]\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(train_data[feature_cols], train_data[target_cols], test_size=0.2, shuffle=True, random_state=42)\n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Validation data shape: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc2d7a3",
   "metadata": {},
   "source": [
    "### 2. Domain-Driven Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1b2b85",
   "metadata": {},
   "source": [
    "#### 2.1 Weighted Component Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd0975e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weighted_properties(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create weighted properties based on component fractions and properties.\n",
    "    Each property is calculated as the sum of the product of each component's fraction and its corresponding\n",
    "    property value.\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing component fractions and properties.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with weighted properties added.\n",
    "    \"\"\"\n",
    "    weighted_features = pd.DataFrame(index=df.index)\n",
    "    for prop in range(1, 11):  # 10 properties\n",
    "        weighted_col = f'Weighted_Property{prop}'\n",
    "        weighted_features[weighted_col] = 0\n",
    "        for comp in range(1, 6):  # 5 components\n",
    "            frac_col = f'Component{comp}_fraction'\n",
    "            prop_col = f'Component{comp}_Property{prop}'\n",
    "            weighted_features[weighted_col] += df[frac_col] * df[prop_col]\n",
    "    return weighted_features\n",
    "\n",
    "# Create weighted properties for training and validation sets\n",
    "X_train = pd.concat([X_train, create_weighted_properties(X_train)], axis=1)\n",
    "X_val = pd.concat([X_val, create_weighted_properties(X_val)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75479c63",
   "metadata": {},
   "source": [
    "#### 2.2 Aggregates, Interactions, Dominance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b18bf545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1600, 157), (400, 157))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_domain_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    new_feats = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # 1. Property Aggregates (mean, min, max, std) for each property across components\n",
    "    for prop in range(1, 11):\n",
    "        prop_cols = [f'Component{comp}_Property{prop}' for comp in range(1, 6)]\n",
    "        new_feats[f'Property{prop}_mean'] = df[prop_cols].mean(axis=1)\n",
    "        new_feats[f'Property{prop}_min'] = df[prop_cols].min(axis=1)\n",
    "        new_feats[f'Property{prop}_max'] = df[prop_cols].max(axis=1)\n",
    "        new_feats[f'Property{prop}_std'] = df[prop_cols].std(axis=1)\n",
    "        new_feats[f'Property{prop}_sum'] = df[prop_cols].sum(axis=1)\n",
    "        new_feats[f'Property{prop}_range'] = new_feats[f'Property{prop}_max'] - new_feats[f'Property{prop}_min']\n",
    "\n",
    "    # 2. Fraction Interactions (pairwise products)\n",
    "    for i in range(1, 6):\n",
    "        for j in range(i+1, 6):\n",
    "            new_feats[f'Frac{ i }x{ j }'] = df[f'Component{i}_fraction'] * df[f'Component{j}_fraction']\n",
    "\n",
    "    # 3. Fraction Ratios (avoid division by zero)\n",
    "    for i in range(1, 6):\n",
    "        for j in range(1, 6):\n",
    "            if i != j:\n",
    "                new_feats[f'Frac{ i }_over_{ j }'] = df[f'Component{i}_fraction'] / (df[f'Component{j}_fraction'] + 1e-6)\n",
    "\n",
    "    # 4. Component Dominance (index of max fraction)\n",
    "    frac_cols = [f'Component{comp}_fraction' for comp in range(1, 6)]\n",
    "    new_feats['Dominant_Component'] = df[frac_cols].idxmax(axis=1).str.extract(r'(\\d+)').astype(int)\n",
    "\n",
    "    # 5. Count of components with fraction > 0.2\n",
    "    new_feats['Num_Components_gt_0.2'] = (df[frac_cols] > 0.2).sum(axis=1)\n",
    "\n",
    "    return new_feats\n",
    "\n",
    "# Add new features to X_train and X_val\n",
    "X_train = pd.concat([X_train, add_domain_features(X_train)], axis=1)\n",
    "X_val = pd.concat([X_val, add_domain_features(X_val)], axis=1)\n",
    "\n",
    "# Final data shapes\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24001753",
   "metadata": {},
   "source": [
    "### 3. Save the Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af723c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed datasets saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Directory of the datasets\n",
    "data_path = Path('../processed_data')\n",
    "\n",
    "# Save the processed datasets\n",
    "X_train.to_csv(data_path / 'X_train.csv', index=False)\n",
    "X_val.to_csv(data_path / 'X_val.csv', index=False)\n",
    "y_train.to_csv(data_path / 'y_train.csv', index=False)\n",
    "y_val.to_csv(data_path / 'y_val.csv', index=False)\n",
    "\n",
    "print(\"Processed datasets saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ab50e0",
   "metadata": {},
   "source": [
    "## Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e7353d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed test dataset saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the raw test dataset\n",
    "test_data = None\n",
    "try:\n",
    "    test_data = pd.read_csv(data_path / \"test.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found, recheck the directory or change the path of directory.\")\n",
    "\n",
    "if test_data is not None:\n",
    "    # Feature columns for test data\n",
    "    test_feature_cols = [col for col in test_data.columns if 'Component' in col]\n",
    "\n",
    "    # Create weighted properties for test data\n",
    "    test_data = pd.concat([test_data, create_weighted_properties(test_data)], axis=1)\n",
    "\n",
    "    # Add domain features to test data\n",
    "    test_data = pd.concat([test_data, add_domain_features(test_data)], axis=1)\n",
    "\n",
    "    # Save the processed test dataset\n",
    "    test_data.to_csv(data_path / 'X_test.csv', index=False)\n",
    "    print(\"Processed test dataset saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
