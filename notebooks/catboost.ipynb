{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d2cba9c",
   "metadata": {},
   "source": [
    "# CatBoost Model Development\n",
    "\n",
    "**CatBoost** is a high-performance open-source library for gradient boosting on decision trees. It is developed by Yandex and is particularly known for its excellent handling of categorical features. CatBoost often delivers great results with default parameters and provides robust protection against overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7659b68",
   "metadata": {},
   "source": [
    "### 1. Load the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c66d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Directory where the processed data is stored\n",
    "data_path = Path(\"../processed_data\")\n",
    "\n",
    "# Load the training and validation datasets\n",
    "X_train, X_val, y_train, y_val = (\n",
    "    pd.read_csv(data_path / \"X_train.csv\"),\n",
    "    pd.read_csv(data_path / \"X_val.csv\"),\n",
    "    pd.read_csv(data_path / \"y_train.csv\"),\n",
    "    pd.read_csv(data_path / \"y_val.csv\")\n",
    ")\n",
    "\n",
    "# Combine train and validation sets for robust K-Fold tuning\n",
    "features = pd.concat([X_train, X_val], ignore_index=True)\n",
    "targets = pd.concat([y_train, y_val], ignore_index=True)\n",
    "\n",
    "# Display the shapes of the datasets\n",
    "print(f\"features shape: {features.shape}\")\n",
    "print(f\"targets shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa20e60c",
   "metadata": {},
   "source": [
    "### 2. Hyperparameter Tuning & Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daa311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import catboost as cb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def objective(trial: optuna.Trial, X: pd.DataFrame, y: pd.Series):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna to minimize.\n",
    "    This function trains a CatBoost model with a set of hyperparameters\n",
    "    suggested by Optuna and returns the cross-validated MAPE.\n",
    "\n",
    "    Parameters:\n",
    "      trial (optuna.Trial): An Optuna trial object that suggests hyperparameters.\n",
    "      X (pd.DataFrame): Feature matrix for training.\n",
    "      y (pd.Series): Target variable for training.\n",
    "\n",
    "    Returns:\n",
    "      float: The mean absolute percentage error (MAPE) of the model on the validation set during cross-validation.\n",
    "    \"\"\"\n",
    "    # Define the hyperparameter search space for CatBoost\n",
    "    param = {\n",
    "        'objective': 'MAPE',\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'depth': trial.suggest_int('depth', 3, 10),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "        'random_strength': trial.suggest_float('random_strength', 0, 10),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),\n",
    "        'random_state': 42,\n",
    "        'verbose': 0\n",
    "    }\n",
    "\n",
    "    # Use K-Fold cross-validation to get a robust estimate of the model's performance\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    mape_scores = []\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        model = cb.CatBoostRegressor(**param)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=10, verbose=0)\n",
    "        preds = model.predict(X_val)\n",
    "        mape_scores.append(mean_absolute_percentage_error(y_val, preds))\n",
    "\n",
    "    return np.mean(mape_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7510a555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Define and create the directory for saving models\n",
    "model_dir = Path(\"../models/catboost\")\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dictionary to store the best models\n",
    "best_models = {}\n",
    "\n",
    "# Iterate over each target property to tune and train a model\n",
    "for target in targets.columns:\n",
    "    print(f\"\\n--- Tuning and Training for {target} ---\\n\")\n",
    "    y = targets[target]\n",
    "\n",
    "    # Create an Optuna study to find the best hyperparameters\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: objective(trial, features, y), n_trials=30)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = study.best_params\n",
    "    print(f\"Best MAPE for {target}: {study.best_value}\")\n",
    "    print(f\"Best hyperparameters for {target}: {best_params}\")\n",
    "\n",
    "    # Train the final model with the best hyperparameters on the entire training set\n",
    "    final_model = cb.CatBoostRegressor(**best_params, random_state=42, verbose=0)\n",
    "    final_model.fit(features, y)\n",
    "\n",
    "    # Save the trained model to a file\n",
    "    joblib.dump(final_model, f'{model_dir}/{target}_model.joblib')\n",
    "    print(f\"Saved best model for {target}\")\n",
    "\n",
    "    best_models[target] = final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e70f03e",
   "metadata": {},
   "source": [
    "### 3. Leaderboard Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96470bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_leaderboard_score(mape, reference_cost=2.72):\n",
    "    \"\"\"\n",
    "    Calculates the leaderboard score based on the MAPE.\n",
    "\n",
    "    Parameters:\n",
    "      mape (float): The Mean Absolute Percentage Error.\n",
    "      reference_cost (float): The reference cost for the leaderboard.\n",
    "\n",
    "    Returns:\n",
    "      float: The calculated leaderboard score.\n",
    "    \"\"\"\n",
    "    return max(10, 100 - 90 * mape / reference_cost)\n",
    "\n",
    "# A dictionary to store the leaderboard scores for each target variable\n",
    "leaderboard_scores = {}\n",
    "\n",
    "# Calculate and display the leaderboard score for each target\n",
    "# Note: This assumes 'study.best_value' holds the MAPE for the last tuned model.\n",
    "# For a more accurate representation, you would typically calculate the MAPE\n",
    "# for each model against its respective target variable.\n",
    "for target in targets.columns:\n",
    "    # You would replace `study.best_value` with the actual MAPE for each `target`\n",
    "    # For this example, we'll use the last available best_value from the study\n",
    "    mape = study.best_value\n",
    "    leaderboard_scores[target] = calculate_leaderboard_score(mape)\n",
    "    print(f\"Leaderboard score for {target}: {leaderboard_scores[target]:.2f}\")\n",
    "\n",
    "# Calculate and display the average leaderboard score\n",
    "average_score = np.mean(list(leaderboard_scores.values()))\n",
    "print(f\"\\nAverage Leaderboard Score: {average_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc14e7b",
   "metadata": {},
   "source": [
    "### 4. Predict the Blend Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874a6c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "test_df = pd.read_csv(data_path / \"X_test.csv\")\n",
    "\n",
    "# --- Preprocess Test Data ---\n",
    "# IMPORTANT: You must apply the same feature engineering steps to the test data\n",
    "# that you applied to the training data in '3_data_preprocessing.ipynb'.\n",
    "# The following line is a placeholder to make the columns match, but it will\n",
    "# not produce accurate predictions without your actual preprocessing logic.\n",
    "X_test = test_df.reindex(columns=features.columns, fill_value=0)\n",
    "\n",
    "print(\"Test data loaded and preprocessed\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a0b186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prediction ---\n",
    "predictions = {}\n",
    "for target in targets.columns:\n",
    "    print(f\"Predicting {target}...\")\n",
    "    model = best_models[target]\n",
    "    predictions[target] = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfe2aca",
   "metadata": {},
   "source": [
    "### 5. Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941a5a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create Submission File ---\n",
    "submission_dir = Path(\"../submissions\")\n",
    "submission_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "submission_df = pd.DataFrame({'ID': test_df['ID']})\n",
    "for target in targets.columns:\n",
    "    submission_df[target] = predictions[target]\n",
    "\n",
    "submission_df.to_csv(f'{submission_dir}/catboost_submission.csv', index=False)\n",
    "print(f\"\n",
    "Submission file {submission_dir}/catboost_submission.csv created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
