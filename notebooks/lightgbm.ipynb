{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffbbf006",
   "metadata": {},
   "source": [
    "# LightGBM Model Development\n",
    "\n",
    "**LightGBM** is a fast, distributed, high-performance gradient boosting framework based on decision tree algorithms. It is designed for efficiency and scalability, making it suitable for large datasets and high-dimensional data. LightGBM supports parallel and GPU learning, handles categorical features natively, and often achieves state-of-the-art results in machine learning competitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ec6063",
   "metadata": {},
   "source": [
    "### 1. Load the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdde9b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Directory where the processed data is stored\n",
    "data_path = Path(\"../processed_data\")\n",
    "\n",
    "# Load the training and validation datasets\n",
    "X_train, X_val, y_train, y_val = (\n",
    "    pd.read_csv(data_path / \"X_train.csv\"),\n",
    "    pd.read_csv(data_path / \"X_val.csv\"),\n",
    "    pd.read_csv(data_path / \"y_train.csv\"),\n",
    "    pd.read_csv(data_path / \"y_val.csv\")\n",
    ")\n",
    "\n",
    "# Dislay the shapes of the datasets\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518cd90c",
   "metadata": {},
   "source": [
    "### 2. Hyperparameter Tuning & Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86d6ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def objective(trial, X, y):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna to minimize.\n",
    "    This function trains a LightGBM model with a set of hyperparameters\n",
    "    suggested by Optuna and returns the cross-validated MAPE.\n",
    "    \"\"\"\n",
    "    # Define the hyperparameter search space for LightGBM\n",
    "    param = {\n",
    "        'objective': 'regression_l1',\n",
    "        'metric': 'mape',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'verbose':-1\n",
    "    }\n",
    "\n",
    "    # Use K-Fold cross-validation to get a robust estimate of the model's performance\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    mape_scores = []\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        model = lgb.LGBMRegressor(**param)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n",
    "                  callbacks=[lgb.early_stopping(10, verbose=False)])\n",
    "        preds = model.predict(X_val)\n",
    "        mape_scores.append(mean_absolute_percentage_error(y_val, preds))\n",
    "\n",
    "    return np.mean(mape_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4484ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import joblib\n",
    "\n",
    "# Dictionary to store the best models\n",
    "best_models = {}\n",
    "\n",
    "# Iterate over each target property to tune and train a model\n",
    "for target in blend_properties:\n",
    "    print(f\"--- Tuning and Training for {target} ---\")\n",
    "    y = train_df[target]\n",
    "\n",
    "    # Create an Optuna study to find the best hyperparameters\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: objective(trial, X, y), n_trials=50) # 50 trials for a good balance of speed and performance\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = study.best_params\n",
    "    print(f\"Best MAPE for {target}: {study.best_value}\")\n",
    "    print(f\"Best hyperparameters for {target}: {best_params}\")\n",
    "\n",
    "    # Train the final model with the best hyperparameters on the entire training set\n",
    "    final_model = lgb.LGBMRegressor(**best_params, random_state=42, n_jobs=-1)\n",
    "    final_model.fit(X, y)\n",
    "\n",
    "    # Save the trained model to a file\n",
    "    joblib.dump(final_model, f'models/{target}_model.joblib')\n",
    "    print(f\"Saved best model for {target}\")\n",
    "\n",
    "    best_models[target] = final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe0a2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prediction ---\n",
    "predictions = {}\n",
    "for target in blend_properties:\n",
    "    print(f\"Predicting {target}...\")\n",
    "    model = best_models[target]\n",
    "    predictions[target] = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b9f2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create Submission File ---\n",
    "submission_df = pd.DataFrame({'ID': test_df['ID']})\n",
    "for target in blend_properties:\n",
    "    submission_df[target] = predictions[target]\n",
    "\n",
    "submission_df.to_csv('submission_v2.csv', index=False)\n",
    "print(\"\\nSubmission file 'submission_v2.csv' created successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
